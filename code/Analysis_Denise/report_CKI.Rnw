
\documentclass{article}

%% Load LaTeX packages
\usepackage{pdflscape}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[authoryear]{natbib}
\usepackage[section]{placeins}
\usepackage{fullpage}
\usepackage{indentfirst}
\usepackage[singlelinecheck=false]{caption}
\usepackage{underscore}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{dcolumn}
\usepackage{xcolor}
\usepackage{newverbs}
\usepackage{fixltx2e}
\usepackage{textcomp}
\usepackage{soul}

\setlength{\parskip}{1em}

% Set up a highlighted typewriter font using \hltexttt{}
\definecolor{Light}{gray}{.93}
\sethlcolor{Light}
\let\OldTexttt\texttt
\newcommand{\hltexttt}[1]{\texttt{\hl{#1}}}

\begin{document}

\begin{titlepage}
\begin{center}

\vspace*{2cm}
{\huge Benchmarking tools and parameters for the amplicon pre-processing pipeline}
\par
\vspace{2cm}
{\large Prepared by: Denise Anderson}
\par
\vspace{0.5cm}
{\large INSiGENe Pty Ltd}
\par
\vspace{0.5cm}
Report Generated: \today
\par
\vspace{4.5cm}
\includegraphics[width=5cm]{logo.png}

\end{center}
\end{titlepage} 

\tableofcontents

\newpage

\section{Description}

Perform analyses and produce plots to compare various tools and parameter settings used in the amplicon pre-processing pipeline. One of the tools being used in the pipeline is DADA2, which models and corrects Illumina-sequenced amplicon errors \citep{pmid27214047}. There are three different methods that can be used for processing the samples:

\begin{itemize}
\item \hltexttt{independent} processes the samples independently
\item \hltexttt{pooled} in which all samples are pooled together for sample inference
\item \hltexttt{pseudo} in which samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time

This study will assess the impacts of each of these processing methods.

\end{itemize}

%% Set knitr global options
<<GlobalOpts, include=FALSE>>=
opts_chunk$set(comment = "", fig.path = "figure/", fig.align = "center", tidy = TRUE, tidy.opts = list(width.cutoff=60))
opts_knit$set(root.dir  ="C:/Users/denia/Sync/Denise/Projects/OceanOmics/amplicon_pipeline")

#### Set chunk options ####
options(scipen = 1, digits = 3)
@

%% Load and cite R libraries
<<LoadLibraries, include=FALSE>>=
library("repmis")
LoadandCite(pkgs=c("repmis", "knitr", "tinytex", "phyloseq", "ggplot2", "zCompositions", "propr", "compositions", "ggfortify", "ALDEx2", "EnhancedVolcano", "plyr", "microViz", "decontam", "patchwork", "readxl", "DivNet", "dplyr"), file = "reports/packages.bib")
@

<<echo=FALSE, eval=TRUE>>=

load("reports/amplicon_pipeline_CKI.RData")
@

<<echo=FALSE, eval=FALSE>>=
pca_biplot <- function(pca_data, phyloseq, colour, colour_label, shape, shape_label, title) {
  long_arrows <- names(which(abs(pca_data$rotation[ ,"PC1"]) > 0.1 | abs(pca_data$rotation[ ,"PC2"]) > 0.1))
  loadings_label <- tax_table(phyloseq)@.Data[ , "LCA_ASV"]
  loadings_label[!names(loadings_label) %in% long_arrows] <- ""

  biplot <- autoplot(pca_data,
                      data = sample_data(phyloseq),
                      colour = colour, shape = shape,
                      label = TRUE, label.label = sample_data(phyloseq)$Sample,
                      label.size = 4, label.vjust = -0.5,
                      loadings = TRUE, loadings.colour = "gray80", loadings.label = TRUE,
                      loadings.label.label = unname(loadings_label),
                      loadings.label.size = 3, loadings.label.colour = "gray30", loadings.label.repel = TRUE) + theme_bw() + guides(colour = guide_legend(title = colour_label), shape = guide_legend(title = shape_label)) + theme(legend.title = element_text(size = 12), legend.text = element_text(size = 11)) + ggtitle(label = title)
  
  return(biplot)
}

pca_biplot_unlabelled <- function(pca_data, phyloseq, size, colour, colour_label, shape, shape_label, title) {
  long_arrows <- names(which(abs(pca_data$rotation[ ,"PC1"]) > 0.1 | abs(pca_data$rotation[ ,"PC2"]) > 0.1))
  loadings_label <- tax_table(phyloseq)@.Data[ , "LCA_ASV"]
  loadings_label[!names(loadings_label) %in% long_arrows] <- ""

  biplot <- autoplot(pca_data,
                      data = sample_data(phyloseq),
                      colour = colour, size = size, shape = shape,
                      loadings = TRUE, loadings.colour = "gray80", loadings.label = TRUE,
                      loadings.label.label = unname(loadings_label),
                      loadings.label.size = 3, loadings.label.colour = "gray30", loadings.label.repel = TRUE) + theme_bw() + guides(colour = guide_legend(title = colour_label), shape = guide_legend(title = shape_label)) + theme(legend.title = element_text(size = 12), legend.text = element_text(size = 11)) + ggtitle(label = title)
  
  return(biplot)
}
@

\section{Pre-processing and exploratory data analysis}

Read in the metadata and processed sequencing data and filter out ASVs that are not observed in at least 3 samples. This minimal filtering removes 122/284 ASVs for the \hltexttt{independent}, 22/321 ASVs for the \hltexttt{pooled} and 90/294 ASVs for the \hltexttt{pseudo} processing.

ASV abundance for each sample can be accessed using \hltexttt{otu_table()}, where ASVs are the rows and samples are the columns. The table of taxonomic names can be accessed using \hltexttt{tax_table()}, where ASVs are the rows and taxonomic classifications are the columns. The table of sample level variables can be accessed using \hltexttt{sample_data()}, where samples are the rows and sample variables are the columns.

<<eval=FALSE, message=FALSE>>=
metadata <- data.frame(read_excel(path = "data/processed/V10_CKI_eDNA_metadata.xlsx"))

samples <- list()
samples$cki_ind <- readRDS(file = "data/processed/CoCosV10I_16S_phyloseq_nt_FALSE.rds")
samples$cki_pooled <- readRDS(file = "data/processed/CoCosV10I_16S_phyloseq_nt_TRUE.rds")
samples$cki_pseudo <- readRDS(file = "data/processed/CoCosV10I_16S_phyloseq_nt_pseudo.rds")

samples <- lapply(samples, FUN = function(y) {sample_data(y)$Sample <- gsub(pattern = "V10_CKI_", replacement = "", x = rownames(sample_data(y))); return(y)})
samples <- lapply(samples, FUN = function(x) tax_mutate(x, LCA_ASV = paste0(unname(tax_table(x)@.Data[, "LCA"]), " (", rownames(tax_table(x)), ")")))
samples <- lapply(samples, FUN = function(x) subset_samples(x, !(replicate_id %in% c("BC", "DI"))))
samples <- lapply(samples, FUN = function(x) {sample_data(x)$Control <- ifelse(sample_data(x)$replicate_id %in% c("EB", "WC"), TRUE, FALSE); return(x)})
samples <- lapply(samples, FUN = function(x) filter_taxa(x, flist = function(y) sum(y >= 1) >= 3, prune = TRUE))
samples <- lapply(samples, FUN = function(x) {colnames(sample_data(x))[2] <- "sampling_method"; return(x)})
samples <- lapply(samples, FUN = function(x) {sample_data(x)$EEZ <- metadata$EEZ[match(rownames(sample_data(x)), metadata$sample_id)]; return(x)})
samples <- lapply(samples, FUN = function(x) {sample_data(x)$Project <- metadata$Project[match(rownames(sample_data(x)), metadata$sample_id)]; return(x)})
@

\subsection{Decontamination of samples}

There are two major sources of contamination that may arise:

\begin{enumerate}
\item External contamination from the sampling environment, the sample collection instruments, the person(s) handling the samples, laboratory surfaces, air and reagents.
\item Internal or cross-contamination, where samples mix during sample processing or sequencing.
\end{enumerate}

The decontam package \citep{pmid30558668}, identifies and removes external contaminants. Decontam implements two simple de novo classification methods based on widely reproduced signatures of external contamination:

\begin{enumerate}
\item Sequences from contaminating taxa are likely to have \textit{frequencies} that inversely correlate with sample DNA concentration.
\item Sequences from contaminating taxa are likely to have higher \textit{prevalence} in control samples than in true samples.
\end{enumerate}

Frequency-based contaminant identification relies on auxiliary DNA quantitation data and prevalence-based contaminant identification relies on sequenced negative controls. Decontam is not intended to detect cross-contamination. For this dataset, we have sequenced negative controls but not DNA quantitation data, so we will use prevalence-based contaminant identification. At least five negative controls are recommended for prevalence-based contaminant identification, but more than this will result in more accurate results. Further recommendations under GitHub issues are to decontaminate all data in a single sequencing batch together, using all applicable controls for increased power.

Run decontam on the samples using the extraction blank and water rinse control samples and remove any contaminating ASVs. Figure \ref{decontam} shows the estimates from decontam, which are the probability of an ASV not being a contaminant. It can be seen that using the DADA2 \hltexttt{pooled} and \hltexttt{pseudo} processing (Figure \ref{decontam}) results in ASVs with higher prevalence (i.e. being called across more samples) when compared to the \hltexttt{independent} approach. A threshold needs to be chosen to define an ASV as a contaminant. The authors state that it is usually straightforward to choose a threshold from the plot, as the probabilities typically separate into a bimodal distribution of ``low'' and ``high'' scores (see some examples \href{https://github.com/benjjneb/decontam/issues/41}{here}). Our data does not show a clear bimodal distribution, however the default threhold of 0.2 looks suitable for filtering out potential contaminants, as there is an increase in the number of prevalent ASVs (i.e. observed in \textgreater 10 samples) from this point (Figure \ref{decontam}). Using this threshold, removes 23/162 ASVs for the \hltexttt{independent}, 36/299 ASVs for the \hltexttt{pooled} and 35/204 ASVs for the \hltexttt{pseudo} processed samples. 

After removing contaminant ASVs, filter out samples that have less than 1,000 reads. This filtering removes 13/140 samples for the \hltexttt{independent}, 10/140 samples for the \hltexttt{pooled} and 15/140 samples for the \hltexttt{pseudo} processed samples. The final numbers for downstream analysis are 139 ASVs and 127 samples for the \hltexttt{independent}, 263 ASVs and 130 samples for the \hltexttt{pooled} and 169 ASVs and 125 samples for the \hltexttt{pseudo} processed samples.

<<eval=FALSE, warning=FALSE>>=
samples_contam <- lapply(samples, FUN = function(x) isContaminant(x, method = "prevalence", neg = "Control", threshold = 0.2))
samples_contam <- lapply(samples_contam, FUN = function(x) {x$Prevalence <- cut(x$prev, breaks = c(3, 5, 10, max(x$prev)), labels = c(">=3 and <=5", ">5 and <=10", ">10"), include.lowest = TRUE); return(x)})
samples <- mapply(names(samples), FUN = function(x) tax_mutate(samples[[x]], Contamination = as.character(samples_contam[[x]]$contaminant)))
samples <- lapply(samples, FUN = function(x) subset_taxa(x, Contamination == FALSE))
samples <- lapply(samples, FUN = function(x) subset_samples(x, !(replicate_id %in% c("EB", "WC"))))
samples <- lapply(samples, FUN = function(x) subset_samples(x, !is.na(EEZ)))
samples <- lapply(samples, FUN = function(y) prune_samples(samples = sample_sums(y) >= 1000, x = y))
samples <- lapply(samples, FUN = function(x) {sample_data(x)$site_id <- factor(as.numeric(sample_data(x)$site_id)); return(x)})
@

<<eval=FALSE, echo=FALSE>>=
saveRDS(samples$cki_ind, file = "data/processed/decontaminated/CoCosV10I_16S_phyloseq_nt_FALSE_decontaminated.rds")
saveRDS(samples$cki_pooled, file = "data/processed/decontaminated/CoCosV10I_16S_phyloseq_nt_TRUE_decontaminated.rds")
saveRDS(samples$cki_pseudo, file = "data/processed/decontaminated/CoCosV10I_16S_phyloseq_nt_pseudo_decontaminated.rds")
@

\begin{figure}
<<echo=FALSE>>=
p1 <- ggplot(samples_contam$cki_ind, aes(x = p, fill = Prevalence)) + geom_histogram(colour = "black", breaks = seq(from = 0, to = 1, by = 0.05), alpha = 0.5) + geom_vline(xintercept = 0.2, colour = "red") + xlab("Probability") + ylab("Count") + scale_y_continuous(expand = c(0, 0)) + scale_x_continuous(expand = c(0, 0))
p2 <- ggplot(samples_contam$cki_pooled, aes(x = p, fill = Prevalence)) + geom_histogram(colour = "black", breaks = seq(from = 0, to = 1, by = 0.05), alpha = 0.5) + geom_vline(xintercept = 0.2, colour = "red") + xlab("Probability") + ylab("Count") + scale_y_continuous(expand = c(0, 0)) + scale_x_continuous(expand = c(0, 0))
p3 <- ggplot(samples_contam$cki_pseudo, aes(x = p, fill = Prevalence)) + geom_histogram(colour = "black", breaks = seq(from = 0, to = 1, by = 0.05), alpha = 0.5) + geom_vline(xintercept = 0.2, colour = "red") + xlab("Probability") + ylab("Count") + scale_y_continuous(expand = c(0, 0)) + scale_x_continuous(expand = c(0, 0))

p1 + p2 + p3 + plot_layout(guides = "collect", ncol = 2) + plot_annotation(tag_levels = "A")
rm(p1, p2, p3)
@
\caption{Estimated probabilities from decontam of whether ASV is not a contaminant for samples processed using DADA2 \hltexttt{independent} (A),  \hltexttt{pooled} (B) and \hltexttt{pseudo} (C) processing. Histogram bars are colour coded by prevalence of the ASV across all samples. The vertical red line indicates the filtering threshold of 0.2 and this will remove 23/162, 36/299 and 35/204 ASVs from analysis respectively.}
\label{decontam}
\end{figure}

\FloatBarrier

\subsection{Exploratory data analysis of decontaminated samples}

High throughput sequencing (HTS) data is compositional, meaning that the abundance of any one nucleotide fragment is only interpretable relative to another. This property emerges from the sequencer itself, which can only sequence a fixed number of nucleotide fragments. Consequently, the final number of fragments sequenced is constrained to an arbitrary limit so that doubling the input material does not double the total number of counts. This constraint also means that an increase in the presence of any one nucleotide fragment necessarily decreases the observed abundance of all other transcripts \citep{pmid31544212}.

Compositional data only carry relative information. Analysing relative data as if it were absolute can yield erroneous results for several common techniques. First, statistical models that assume independence between features are flawed because of the mutual dependency between components. Second, distances between samples are misleading and erratically sensitive to the arbitrary inclusion or exclusion of components. Third, components can appear definitively correlated even when they are statistically independent. For these reasons, compositional data pose specific challenges to the differential expression, clustering and correlation analyses routinely applied to HTS data.

There are three general approaches to analysing compositional data. First, the ``normalisation-dependent'' approach seeks to normalise the data in order to reclaim absolute abundances. However, normalisations depend on assumptions that may not hold true outside of tightly controlled experiments. Second, the ``transformation-dependent'' approach transforms the data with regard to a reference to make statistical inferences relative to the chosen reference. Third, the ``transformation-independent'' approach performs calculations directly on the components or component ratios. The latter two approaches constitute compositional data analysis (CoDA).

The three major objectives that can be addressed using compositional data analysis are \citep{pmid27314511}:

\begin{enumerate}
\item Do the data show any structure? That is, do the data partition into groups?
\item What is the difference between groups? This can be between pre-defined groups or groups identified during exploratory data analysis.
\item What is the correlation structure of the taxonomic groups? Do any of the taxa correlate with the metadata?
\end{enumerate}

CoDA methods depend on logarithms that do not compute for zeros. Three types of zeros exist: 1) ``rounding'', also called ``sampling'', where the feature exists in the sample below the detection limit, 2) ``count'', where the feature exists in the sample, but counting is not exhaustive enough to see it at least once, and 3) ``essential'', where the feature does not exist in the sample at all. The approach to zero handling depends on the nature of the zeros. For HTS data, a nucleotide fragment is either sequenced or not, and would not contain ``rounding'' zeros. Because there is no general methodology for dealing with ``essential'' zeros within a strict CoDA framework, we assume that any feature present in at least one sample could appear in another sample if sequenced with infinite depth, and thus treat all HTS zeros as ``count'' zeros.

Check the extent of zero counts across the samples and the ASVs on abundance heatmaps (Figures \ref{hm1}, \ref{hm2} and \ref{hm3}).

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=12, fig.width=9>>=
plot_heatmap(samples$cki_ind, taxa.label = "LCA_ASV", low = "lavenderblush", high = "red", na.value = "white", max.label = nrow(otu_table(samples$cki_ind)), title = "DADA2 independent processing") + theme(axis.text.x = element_text(size = 3), axis.text.y = element_text(size = 4))
@
\caption{Heatmap of samples collected during the transect from Fremantle, where rows and columns are organised using NMDS ordination of Bray-Curtis dissimilarities. Zero counts are coloured white and abundances are shown on a log\textsubscript{4} scale.}
\label{hm1}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=12, fig.width=9>>=
plot_heatmap(samples$cki_pooled, taxa.label = "LCA_ASV", low = "lavenderblush", high = "red", na.value = "white", max.label = nrow(otu_table(samples$cki_pooled)), title = "DADA2 pooled processing") + theme(axis.text.x = element_text(size = 3), axis.text.y = element_text(size = 3))
@
\caption{Heatmap of samples collected during the transect from Fremantle, where rows and columns are organised using NMDS ordination of Bray-Curtis dissimilarities. Zero counts are coloured white and abundances are shown on a log\textsubscript{4} scale.}
\label{hm2}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=12, fig.width=9>>=
plot_heatmap(samples$cki_pseudo, taxa.label = "LCA_ASV", low = "lavenderblush", high = "red", na.value = "white", max.label = nrow(otu_table(samples$cki_pseudo)), title = "DADA2 pseudo processing") + theme(axis.text.x = element_text(size = 3), axis.text.y = element_text(size = 4))
@
\caption{Heatmap of samples collected during the transect from Fremantle, where rows and columns are organised using NMDS ordination of Bray-Curtis dissimilarities. Zero counts are coloured white and abundances are shown on a log\textsubscript{4} scale.}
\label{hm3}
\end{figure}

There are two general approaches to zero handling. In ``feature removal'', components with zeros get excluded, yielding a sub-composition that can be analysed by any CoDA method. Feature removal is usually appropriate when a feature contains many zeros and can always be justified for ``essential'' zeros. In ``feature modification'', zeros get replaced with a non-zero value, with or without modification to non-zeros. Results should be checked for sensitivity to the chosen method(s).

For ``count'' zeros, it is recommended to replace zeros using a Bayesian-multiplicative replacement strategy that preserves the ratios between the non-zero components. Replace zero values using the count zero multiplicative method and output pseudo-counts. There was a warning message because many of the samples and ASVs have more than 80\% zeroes as can be seen on the abundance heatmaps.

<<eval=FALSE, warning=FALSE, message=FALSE, results='hide'>>=
samples_zimp <- lapply(samples, FUN = function(x) cmultRepl(t(otu_table(x)), method = "CZM", output = "p-counts"))
@

All components in a composition are mutually dependent features that cannot be understood in isolation. Therefore, any analysis of individual components is done with respect to a reference. This reference transforms each sample into an unbounded space where any statistical method can be used. The centred log ratio (CLR) uses the geometric mean of the sample vector as the reference. The additive log-ratio (ALR) transformation uses a single component as the reference. Other transformations use specialised references based on the geometric mean of a subset of components (collectively called multi-additive log-ratio [MLR] transformations).

Importantly, transformations are not normalisations: while normalisations claim to recast the data in absolute terms, transformations do not. The results of a transformation-based analysis must be interpreted with respect to the chosen reference. Of these, the CLR transformation is most common. CLR transformed data reveals how features behave relative to the per-sample average. Each transformed feature is a log-ratio of the original feature divided by the reference and therefore should get interpreted as a kind of within-sample log-fold difference. Compute the CLR.

<<eval=FALSE, warning=FALSE, message=FALSE, results='hide'>>=
samples_clr <- lapply(samples_zimp, FUN = function(x) propr(counts = x))
@

Perform PCA on the CLR transformed data and plot on covariance biplots.

<<eval=FALSE, warning=FALSE, message=FALSE, results='hide'>>=
samples_pca <- lapply(samples_clr, FUN = function(x) prcomp(x@logratio))
@

Covariance biplots are descriptive and exploratory, but not quantitative unless at least 90\% of the variance is explained on the first two principal components. The ability to examine variation of both the samples and taxa on the same plot provides insights into which taxa are compositionally associated and which taxa are driving (or not) the location of particular samples. There are rules that can be used to interpret the plots, whilst keeping in mind that the rules are less robust as the percentage of variance explained decreases. Quantitative tools should also be applied to support any observations made from the biplot. The rules are based on the information provided \href{https://tspace.library.utoronto.ca/bitstream/1807/72631/1/cjm-2015-0821.R2-supplemental-1.pdf}{here}:

\begin{enumerate}
\item The arrows on the biplot approximate the amount of variance exhibited by each ASV relative to the centre of the dataset, where longer arrows mean more variation across all samples. ASVs with long arrows are represented well, but ASVs with short arrows should be interpreted with care.
\item The location of the sample shows how variable it is relative to other samples.
\item Samples that are highly variable, and that are in the same direction as a long arrow for an ASV, will contain that ASV in high abundance. The inverse is also true.
\item ASVs where the tips of the arrows are co-incident and of the same length indicate that the ratio between those two ASVs are nearly identical across all samples up to the limit of the projection of the data (i.e. the projections can be misleading if almost all of the variation is not explained by the first two principal components).
\item ASVs where the tips of the arrows are very distant from each other, indicate highly variable ratios across the samples.
\item The closer the angle of the arrows between ASVs is to 90 or 270 degrees, the smaller the correlation.
\item ASVs with arrows pointing in the same direction (close to zero degree angle) or in opposite directions (close to 180 degree angle) will be positively or negatively correlated respectively.
\item The angle between arrows contains information about correlations between pairs, or groups of ratios, more formally, the cosine of the angle is proportional to the correlation between the pairs of ratios.
\end{enumerate}

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=10, fig.width=10>>=
plot(pca_biplot_unlabelled(pca_data = samples_pca$cki_ind, phyloseq = samples$cki_ind, size = 3, colour = "site_id", colour_label = "Site identifier", shape = "EEZ", shape_label = "EEZ", title = "DADA2 independent processing"))
@
\caption{Covariance biplot of samples and ASVs taken during the transect from Fremantle. Samples are colour coded by site of collection (inside or outside EEZ). ASVs are labelled in grey with last common ancestor (LCA) and ASV identifier and note some labels are omitted for readability.}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=10, fig.width=10>>=
plot(pca_biplot_unlabelled(pca_data = samples_pca$cki_pooled, phyloseq = samples$cki_pooled, size = 3, colour = "site_id", colour_label = "Site identifier", shape = "EEZ", shape_label = "EEZ", title = "DADA2 pooled processing"))
@
\caption{Covariance biplot of samples and ASVs taken during the transect from Fremantle. Samples are colour coded by site of collection (inside or outside EEZ). ASVs are labelled in grey with last common ancestor (LCA) and ASV identifier and note some labels are omitted for readability.}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE, fig.height=10, fig.width=10>>=
plot(pca_biplot_unlabelled(pca_data = samples_pca$cki_pseudo, phyloseq = samples$cki_pseudo, size = 3, colour = "site_id", colour_label = "Site identifier", shape = "EEZ", shape_label = "EEZ", title = "DADA2 pseudo processing"))
@
\caption{Covariance biplot of samples and ASVs taken during the transect from Fremantle. Samples are colour coded by site of collection (inside or outside EEZ). ASVs are labelled in grey with last common ancestor (LCA) and ASV identifier and note some labels are omitted for readability.}
\end{figure}

\section{DivNet}

DivNet estimates diversity indices using compositional data models \citep{pmid32432696}. DivNet requires a reference taxon, against which other abundances are to be compared. It does this automatically provided that there are some taxon that are observed in all samples. This is not the case for our data, so the reference taxon has to be chosen manually. It was suggested \href{https://github.com/adw96/DivNet/issues/14}{here} that the taxon with median abundance across all samples is good choice and I have implemented this suggestion. Estimates are believed to be very robust to choice of reference taxon, however it is probably worth checking sensitivity to choice of reference taxon.

<<eval=FALSE>>=
divnet_alpha <- lapply(samples, FUN = function(x) divnet(W = x, X = "site_id", base = names(sort(taxa_sums(x))[round(length(taxa_sums(x))/2)])))
@

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
data.frame(divnet_alpha$cki_ind$shannon %>% summary, sample_data(samples$cki_ind)) %>%
  distinct(estimate, error, lower, upper, site_id) %>%
  ggplot(aes(x = site_id, y = estimate, col = site_id)) + geom_point() + geom_segment(aes(x = .data[["site_id"]], xend = .data[["site_id"]], y = .data[["lower"]], yend = .data[["upper"]])) + guides(colour = FALSE) + xlab("Site identifier") + ylab("Shannon entropy estimate") + theme_bw()
@
\caption{DivNet estimates of Shannon entropy and associated confidence interval for the CKI sites. Samples were processed using the DADA2 \hltexttt{independent} approach.}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
data.frame(divnet_alpha$cki_pooled$shannon %>% summary, sample_data(samples$cki_pooled)) %>%
  distinct(estimate, error, lower, upper, site_id) %>%
  ggplot(aes(x = site_id, y = estimate, col = site_id)) + geom_point() + geom_segment(aes(x = .data[["site_id"]], xend = .data[["site_id"]], y = .data[["lower"]], yend = .data[["upper"]])) + guides(colour = FALSE) + xlab("Site identifier") + ylab("Shannon entropy estimate") + theme_bw()
@
\caption{DivNet estimates of Shannon entropy and associated confidence interval for the CKI sites. Samples were processed using the DADA2 \hltexttt{pooled} approach.}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
data.frame(divnet_alpha$cki_pseudo$shannon %>% summary, sample_data(samples$cki_pseudo)) %>%
  distinct(estimate, error, lower, upper, site_id) %>%
  ggplot(aes(x = site_id, y = estimate, col = site_id)) + geom_point() + geom_segment(aes(x = .data[["site_id"]], xend = .data[["site_id"]], y = .data[["lower"]], yend = .data[["upper"]])) + guides(colour = FALSE) + xlab("Site identifier") + ylab("Shannon entropy estimate") + theme_bw()
@
\caption{DivNet estimates of Shannon entropy and associated confidence interval for the CKI sites. Samples were processed using the DADA2 \hltexttt{pseudo} approach.}
\end{figure}

<<echo=FALSE, eval=FALSE>>=
save.image("reports/amplicon_pipeline_CKI.RData", compress = FALSE)
@

\section{Session Information}
<<sessionInfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\section{References}
\nocite{*}
\bibliographystyle{jss}
\bibliography{packages,main}

\end{document}